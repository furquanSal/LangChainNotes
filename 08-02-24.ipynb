{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a0fac6",
   "metadata": {},
   "source": [
    "# February 08, 2024 - Session Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8f1ea4",
   "metadata": {},
   "source": [
    "##### 1. Math LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a439283e-b46b-4fcb-b130-33870684fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1abe12f-878e-4c92-aa3b-24ad8668c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMMathChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30169dd-6d95-4461-ae26-fc48b1731672",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_math_model = LLMMathChain.from_llm(llm=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb23e3-04d2-4642-ac7c-759673ad8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_math_model.invoke(\"What is 81 to the power of 7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf05abd3-2e51-4cad-91be-57b3030263de",
   "metadata": {},
   "source": [
    "\n",
    "##### 2. Question - Answering Chains (4 types)\n",
    "\n",
    "A Question-Answering in LangChain is a sequence of components that work together to answer questions based on a given context. The chain typically consists of a retriever component that identifies relevant documents from a corpus, a reader component that extracts information from those documents, and a generator component that synthesizes a response to the question. There are four different types of QA chain. Namely:\n",
    "- load_qa chain\n",
    "- RetrieveQA chain\n",
    "- VectorStoreIndexCreator\n",
    "- ConversationalRetrieval Chain\n",
    "\n",
    "Let us look one by one in detail\n",
    "\n",
    "(a) - `load_qa_chain`: This chain is used to load question-answering model and a vector store. The vector store contains the context documents that the model can use to answer questions. This chain is typically used as a building block for other chains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620bb9c-3bb8-47bb-8023-a8bf3e064c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6b45b1-87c0-4b55-a810-ac9f751e7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat = ChatGoogleGenerativeAI(model=\"gemini-pro\",convert_system_message_to_human=True)\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bf94d6-5118-415f-957b-191a4049fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed_func = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "embed_func = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e6c5d5-46ec-4fd7-a04d-3a1d4c071ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"./Data/generative-ai-on-the-cusp-of-disruption-a-primer.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e245c89e-d3b3-4060-b170-778068ea963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f3b1df-7ad2-40fb-a18f-1f226e9f62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0b80e2-3d77-46db-a855-c3988cd98d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GenAIPrimerDB = Chroma(embedding_function=embed_func, persist_directory=\"./GenAI_Primer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c48891-c0e7-418f-a565-b7b02259fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm=chat,chain_type=\"stuff\",verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d718f-7cf8-478b-957e-bdb16fe1ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"input_documents\":docs, \"question\":\"What are the Enterprise use cases of Generative AI?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93757061-5fcb-4d71-a202-9ada2c427f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6107b-2123-4978-a5c4-d211ee9e7087",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"input_documents\"][1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c64959-727e-4e6c-9919-5ed9b88eca97",
   "metadata": {},
   "source": [
    "(b) - `RetrieveQA` chain: This chain is used to retrieve relevant documents from a vector store based on a given question, and then use a question-answering model to generate an answer from those documents. This chain is useful when you have a large corpus of documents and you want to find the most relevant ones to answer a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019bacca-e4d6-4c9c-97d8-5a1f9944f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf3ca1-aa43-4c95-8eba-637b48e06e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = GenAIPrimerDB.as_retriever(search_type =\"similarity\", search_kwargs={'k':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36126b52-0eee-432f-8247-c80152c0d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=chat, chain_type=\"stuff\",retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112fcc53-79ce-41ef-951d-80787de8a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=chain.invoke(\"What is Lab45?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27be71d-9f1b-4ae4-8591-717b77f2e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ead46-1975-4cb9-afac-052fa37d2afc",
   "metadata": {},
   "source": [
    "(c) - `VectorStoreIndexCreator`: This chain is used to create a vector store from a set of documents. The vector store is a data structure that allows for efficient similarity search, enabling the retrieval of relevant documents based on a given question. This chain is typically used as a preprocessing step for other chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb80a0d-2c01-4ec9-a410-5a0e045ff182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ca1ac-0b74-4215-915c-5feabec05fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(text_splitter=text_splitter, embedding=embed_func, vectorstore_cls=Chroma).from_loaders(loaders=[loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9655c7-c3b6-485d-8d8b-fd00023a04de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (index.query (\"What are the Enterprise use cases of Generative AI?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:langchain] *",
   "language": "python",
   "name": "conda-env-langchain-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
