{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# February 06, 2024 - Session Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Route Documentation\n",
    "\n",
    "#### __(Step 1) - Router Template:__\n",
    "\n",
    "`router_temp` is created by formatting the predefined `MULTI_PROMPT_ROUTER_TEMPLATE` with the `destination_str`.\n",
    "The `MULTI_PROMPT_ROUTER_TEMPLATE` is a template designed for creating router chains that handle multiple prompts or input scenarios.\n",
    "It dynamically specifies the available destinations (subchains) based on the provided `destination_str`.\n",
    "\n",
    "#### __(Step 2) - Router Prompt Template:__\n",
    "\n",
    "`router_prompt` is a dynamic prompt template for the router chain.\n",
    "It uses the formatted `router_temp` as the template.\n",
    "The template expects an input variable named \"input\" (which will be replaced with actual user input during formatting).\n",
    "The `RouterOutputParser()` handles parsing the LLM‚Äôs response based on this prompt.\n",
    "\n",
    "#### __(Step 3) - LLMRouterChain:__\n",
    "`router_chain` is an instance of `LLMRouterChain`.\n",
    "It uses the specified LLM (llm) and the `router_prompt`.\n",
    "The router chain will evaluate user inputs using the LLM and route them to the appropriate subchain based on the LLM‚Äôs output.\n",
    "\n",
    "Use Cases:\n",
    "The resulting `router_chain` can be used in various scenarios:\n",
    "- Chatbots: To handle diverse user queries effectively.\n",
    "- Task Routing: For complex tasks where different subchains specialize in specific types of input.\n",
    "- Dynamic Selection: Based on context or user input, the router chain dynamically selects the most relevant processing path.\n",
    "\n",
    "In summary, this code snippet sets up a router chain that intelligently routes user inputs to different subchains based on the LLM‚Äôs output, enhancing the overall language model interaction experience. üåêüîó\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "LLMRouterChain requires base llm_chain prompt to have an output parser that converts LLM text output to a dictionary with keys 'destination' and 'next_inputs'. Received a prompt with no output parser. (type=value_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.router import RouterChain, MultiRouteChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model='gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scientist = '''\n",
    "You are a scientist.\n",
    "You do not answer any question that is outside the fold of science concepts.\n",
    "Here is the question for you. \\n {input}\n",
    "'''\n",
    "\n",
    "mathematician = '''\n",
    "You are a mathematician.\n",
    "You do not answer any question that is outside the fold of mathematic concepts.\n",
    "Here is the question for you. \\n {input}\n",
    "'''\n",
    "\n",
    "lawyer = '''\n",
    "You are a Lawyer. \n",
    "You do not answer any question that is outside the fold of law concepts.\n",
    "Here is the question for you. \\n {input}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\" : \"scientist\",\n",
    "        \"desc\" : \"Rsponds to science questions only\",\n",
    "        \"prompt_template\" : scientist\n",
    "    },\n",
    "    {\n",
    "        \"name\" : \"mathematician\",\n",
    "        \"desc\" : \"Responds to mathematics questions only\",\n",
    "        \"prompt_template\" : mathematician\n",
    "    },\n",
    "    {\n",
    "        \"name\" : \"Lawyer\",\n",
    "        \"desc\" : \"Responds to law questions only\",\n",
    "        \"prompt_template\" : lawyer\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(template='{input}')\n",
    "default_chain = LLMChain(llm=model, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "\n",
    "for _ in prompt_infos:\n",
    "    name = _['name']\n",
    "    templ = _['prompt_template']\n",
    "    chat_prompt = ChatPromptTemplate.from_template(templ)\n",
    "    chain = LLMChain(llm=model, prompt=chat_prompt)\n",
    "    destination_chains[name] = chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scientist': LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='\\nYou are a scientist.\\nYou do not answer any question that is outside the fold of science concepts.\\nHere is the question for you. \\n {input}\\n'))]), llm=ChatGoogleGenerativeAI(model='gemini-pro', client= genai.GenerativeModel(\n",
       "    model_name='models/gemini-pro',\n",
       "    generation_config={}.\n",
       "    safety_settings={}\n",
       " ))),\n",
       " 'mathematician': LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='\\nYou are a mathematician.\\nYou do not answer any question that is outside the fold of mathematic concepts.\\nHere is the question for you. \\n {input}\\n'))]), llm=ChatGoogleGenerativeAI(model='gemini-pro', client= genai.GenerativeModel(\n",
       "    model_name='models/gemini-pro',\n",
       "    generation_config={}.\n",
       "    safety_settings={}\n",
       " ))),\n",
       " 'Lawyer': LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='\\nYou are a Lawyer. \\nYou do not answer any question that is outside the fold of law concepts.\\nHere is the question for you. \\n {input}\\n'))]), llm=ChatGoogleGenerativeAI(model='gemini-pro', client= genai.GenerativeModel(\n",
       "    model_name='models/gemini-pro',\n",
       "    generation_config={}.\n",
       "    safety_settings={}\n",
       " )))}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#routing destinations\n",
    "destinations = [f\"{_['name']} : {_['desc']}\" for _ in prompt_infos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scientist : Rsponds to science questions only',\n",
       " 'mathematician : Responds to mathematics questions only',\n",
       " 'Lawyer : Responds to law questions only']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations_str = '\\n'.join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_temp = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "scientist : Rsponds to science questions only\n",
      "mathematician : Responds to mathematics questions only\n",
      "Lawyer : Responds to law questions only\n",
      "\n",
      "<< INPUT >>\n",
      "{input}\n",
      "\n",
      "<< OUTPUT (must include ```json at the start of the response) >>\n",
      "<< OUTPUT (must end with ```) >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(router_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt = PromptTemplate(\n",
    "    template=router_temp, \n",
    "    input_variables=['input'], \n",
    "    output_parser=RouterOutputParser()\n",
    "    )\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm=model, prompt=router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiRouteChain(router_chain=router_chain, destination_chains=destination_chains, default_chain=default_chain, verbose=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiRouteChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mfurq\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scientist: {'input': 'What is an atom?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "An atom is the basic unit of matter and the defining structure of elements. It consists of a central nucleus surrounded by electrons. The nucleus is made of positively charged protons and neutral neutrons. The electrons are negatively charged and orbit the nucleus in shells. Different elements have different numbers of protons, which determines their atomic number. The atomic number also determines the number of electrons in an atom, since atoms are electrically neutral. Atoms can combine with each other to form molecules, which are the basic units of compounds.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"What is an atom?\")\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiRouteChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mfurq\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lawyer: {'input': 'What is IPC 421?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "**IPC Section 421:** Dishonestly or fraudulently inducing delivery of property\n",
      "\n",
      "**Ingredients of the offence**:\n",
      "- Dishonest or fraudulent inducement: The accused must have induced the delivery of property by dishonestly or fraudulently. Dishonesty means the intention to deceive or cause wrongful gain or loss. Fraudulently means using deception or trickery to induce the delivery of property.\n",
      "- Delivery of property: The accused must have induced the delivery of property. Property includes movable and immovable property, tangible and intangible property, and actionable claims.\n",
      "- Knowledge or intention: The accused must have known or intended that the delivery of property would be induced by his/her dishonest or fraudulent inducement.\n",
      "\n",
      "**Punishment**:\n",
      "- Imprisonment for a term of up to three years, or\n",
      "- Fine, or\n",
      "- Both\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"What is IPC 421?\")\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiRouteChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mfurq\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mathematician: {'input': 'What is the derivative of x with respect to y?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The derivative of x with respect to y is 0, since x is a constant with respect to y.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"What is derivative of x with respect to y?\")\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<< End Of Document >>>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
