{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8704585b",
   "metadata": {},
   "source": [
    "# January 31, 2024 - Session Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ead7795",
   "metadata": {},
   "source": [
    "##### 1. Loading - Transforming - Embedding - Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da71f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5681f96e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = TextLoader(file_path=\"./Data_Samples/paul_graham_essay.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a97f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa6d09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117daf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=1000, chunk_overlap= 125) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc048f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks_2 = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee3ea76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece59129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks_by_tokens = token_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb443c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ef1ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943f9355",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_embedding_function = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb22a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a104b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading the existing the vectorDB\n",
    "freshly_loaded_donut = Chroma(embedding_function=embedding_function, persist_directory=\"./state_db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a861787",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = freshly_loaded_donut.similarity_search(\"Who ran American office for economic vision?\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f98032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0632914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e4f6c2",
   "metadata": {},
   "source": [
    "##### 2. Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da5a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = freshly_loaded_donut.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2308727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_result= retriever.get_relevant_documents(\"Who ran American office for economic vision?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef3d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_docs (original_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf99808",
   "metadata": {},
   "source": [
    "##### 2(a) Multi Query Retriever\n",
    "\n",
    "A Multi Query Retriever is a technique used in Retrieval-Augmented Generation (RAG) models where an Language Model (LLM) is used automate the process of tuning. This generates multiple queries from different perspectives for a given user input question. The idea is to gather more information and different viewpoints from the documents in the vector store, which can help in providing a more comprehensive and accurate answer. However, this technique can also have drawbacks such as taking longer to execute and costing more due to increased LLM invocations and/or increased token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19783d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = multi_retriever.get_relevant_documents(\"Who ran American office for economic vision?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40904380",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ad437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for printing the docs in an organized manner\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join\n",
    "     ([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342091c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_docs(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc2d694",
   "metadata": {},
   "source": [
    "##### 2(b) ContextualCompressionRetriever\n",
    "\n",
    "`ContextualCompressionRetriever` is a component in LangChain that is used for retrieving compressed context from a database or other storage system. It is designed to work with large language models (LLMs) and can be used to provide context to the language model when generating responses. The retriever can compress and store large amounts of text, such as entire documents or conversations, and then retrieve and decompress the relevant portions when needed. This can be useful for applications that require the language model to have access to a large amount of context, but where storing all of the context in memory is not practical. The `ContextualCompressionRetriever` can be used in conjunction with other LangChain modules, such as the language model integration and prompt handling modules, to create advanced language model applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f724e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe0d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = LLMChainExtractor.from_llm(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4f206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_compressor = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result=contextual_compressor.get_relevant_documents(\"Who ran American office for economic vision?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba78e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_docs(new_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b9691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b869816",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26807cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = HumanMessage(content=\"who is the prime minster of UK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb90ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke([message])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20369591",
   "metadata": {},
   "source": [
    "<<< End Of Document >>>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:langchain] *",
   "language": "python",
   "name": "conda-env-langchain-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
